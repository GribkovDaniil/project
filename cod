# ============================================================================
# BLACKJACK NEURAL NETWORK STRATEGY - –ü–û–õ–ù–´–ô –ö–û–î –î–õ–Ø GOOGLE COLAB
# ============================================================================
# –°–∏—Å—Ç–µ–º–∞ –æ–±—É—á–µ–Ω–∏—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ Blackjack —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º
# –≥–ª—É–±–æ–∫–æ–π –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏ –Ω–∞ TensorFlow –∏ Q-Learning

# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –±–∏–±–ª–∏–æ—Ç–µ–∫
import subprocess
import sys

print("üì¶ –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π...")
subprocess.check_call([sys.executable, "-m", "pip", "install", "-q", "tensorflow>=2.10.0"])
subprocess.check_call([sys.executable, "-m", "pip", "install", "-q", "numpy"])
subprocess.check_call([sys.executable, "-m", "pip", "install", "-q", "matplotlib"])
subprocess.check_call([sys.executable, "-m", "pip", "install", "-q", "seaborn"])
subprocess.check_call([sys.executable, "-m", "pip", "install", "-q", "pandas"])
print("‚úì –ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã!\n")

# ============================================================================
# –ò–ú–ü–û–†–¢–´
# ============================================================================

import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import random
from collections import deque, defaultdict
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
from datetime import datetime
import json
import os

# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ seed –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
np.random.seed(42)
tf.random.set_seed(42)
random.seed(42)

print("=" * 80)
print(" üé∞ BLACKJACK NEURAL NETWORK STRATEGY SYSTEM")
print(" –°–∏—Å—Ç–µ–º–∞ –æ–±—É—á–µ–Ω–∏—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –∏–≥—Ä—ã Blackjack")
print("=" * 80)
print()

# ============================================================================
# –ö–õ–ê–°–°: –°–∏–º—É–ª—è—Ç–æ—Ä –∏–≥—Ä—ã Blackjack
# ============================================================================

class BlackjackGame:
    """
    –°–∏–º—É–ª—è—Ç–æ—Ä –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–æ–π –∏–≥—Ä—ã Blackjack.
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏.
    """

    def __init__(self, num_decks=1):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏–≥—Ä—ã.

        Args:
            num_decks: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö –∫–æ–ª–æ–¥ (1-8)
        """
        self.num_decks = num_decks
        self.reset_deck()

    def reset_deck(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è/–ø–µ—Ä–µ–∏–Ω–∏—Üializ–∞—Ü–∏—è –∫–æ–ª–æ–¥—ã –∫–∞—Ä—Ç."""
        # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –∫–æ–ª–æ–¥–∞: 13 –∫–∞—Ä—Ç * 4 –º–∞—Å—Ç–∏
        deck = [2, 3, 4, 5, 6, 7, 8, 9, 10, 10, 10, 10, 11] * 4 * self.num_decks
        self.deck = deck
        random.shuffle(self.deck)

    def deal_card(self):
        """–†–∞–∑–¥–∞—á–∞ –∫–∞—Ä—Ç—ã –∏–∑ –∫–æ–ª–æ–¥—ã."""
        if len(self.deck) < 10:
            self.reset_deck()
        return self.deck.pop()

    def calculate_hand_value(self, hand):
        """
        –†–∞—Å—á—ë—Ç —Å—Ç–æ–∏–º–æ—Å—Ç–∏ —Ä—É–∫–∏ —Å —É—á—ë—Ç–æ–º –ø—Ä–∞–≤–∏–ª–∞ —Ç—É–∑–∞.

        Returns:
            tuple: (value, has_usable_ace)
        """
        total = sum(hand)
        aces = hand.count(11)

        # –ï—Å–ª–∏ —Å—É–º–º–∞ –±–æ–ª—å—à–µ 21, –ø–µ—Ä–µ—Å—á–∏—Ç—ã–≤–∞–µ–º —Ç—É–∑—ã
        while total > 21 and aces > 0:
            total -= 10  # –¢—É–∑ 11 —Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è 1
            aces -= 1

        has_usable_ace = (aces > 0 and total + 10 <= 21)
        return total, has_usable_ace

    def play_game(self, agent_strategy=None, use_basic_strategy=False):
        """
        –ü—Ä–æ–≤–µ–¥–µ–Ω–∏–µ –æ–¥–Ω–æ–π –∏–≥—Ä—ã Blackjack.

        Args:
            agent_strategy: –§—É–Ω–∫—Ü–∏—è, –∫–æ—Ç–æ—Ä–∞—è –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –¥–µ–π—Å—Ç–≤–∏–µ (0=Stand, 1=Hit)
            use_basic_strategy: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–∏ –±–∞–∑–æ–≤—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é Blackjack

        Returns:
            dict: –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ–± –∏–≥—Ä–µ (—Ä–µ–∑—É–ª—å—Ç–∞—Ç, –¥–µ–π—Å—Ç–≤–∏—è, —Å–æ—Å—Ç–æ—è–Ω–∏—è, –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–µ)
        """
        # –†–∞–∑–¥–∞—á–∞ –Ω–∞—á–∞–ª—å–Ω—ã—Ö –∫–∞—Ä—Ç
        player_hand = [self.deal_card(), self.deal_card()]
        dealer_hand = [self.deal_card(), self.deal_card()]

        game_history = []

        # –•–æ–¥ –∏–≥—Ä–æ–∫–∞
        while True:
            player_value, player_ace = self.calculate_hand_value(player_hand)
            dealer_showing = dealer_hand[0]

            # –ï—Å–ª–∏ —Ä—É–∫–∞ > 21, –∏–≥—Ä–æ–∫ –ø–µ—Ä–µ–±—Ä–∞–ª
            if player_value > 21:
                return {
                    'result': 'BUST',
                    'reward': -1,
                    'player_final': player_value,
                    'dealer_final': None,
                    'history': game_history
                }

            # –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å –¥–µ–π—Å—Ç–≤–∏–µ
            if agent_strategy is None:
                # –°–ª—É—á–∞–π–Ω–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è
                action = random.choice([0, 1])
            elif use_basic_strategy:
                action = self.get_basic_strategy_action(player_value, dealer_showing, player_ace)
            else:
                # –°—Ç—Ä–∞—Ç–µ–≥–∏—è –∞–≥–µ–Ω—Ç–∞
                state = (player_value, dealer_showing, player_ace)
                action = agent_strategy(state)

            # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Å–æ—Å—Ç–æ—è–Ω–∏–µ
            state = (player_value, dealer_showing, player_ace)
            game_history.append((state, action))

            # –ï—Å–ª–∏ –∏–≥—Ä–æ–∫ –≤—ã–±—Ä–∞–ª Stand
            if action == 0:
                break

            # –ò–Ω–∞—á–µ –≤–∑—è—Ç—å –∫–∞—Ä—Ç—É
            player_hand.append(self.deal_card())

        # –•–æ–¥ –¥–∏–ª–µ—Ä–∞ (–¥–∏–ª–µ—Ä –¥–æ–ª–∂–µ–Ω –≤–∑—è—Ç—å –∫–∞—Ä—Ç—É –µ—Å–ª–∏ < 17)
        while True:
            dealer_value, _ = self.calculate_hand_value(dealer_hand)
            if dealer_value >= 17:
                break
            dealer_hand.append(self.deal_card())

        # –†–∞—Å—á—ë—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        player_final, _ = self.calculate_hand_value(player_hand)
        dealer_final, _ = self.calculate_hand_value(dealer_hand)

        # –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å –ø–æ–±–µ–¥–∏—Ç–µ–ª—è
        if dealer_final > 21:
            result = 'WIN'
            reward = 1
        elif player_final > dealer_final:
            result = 'WIN'
            reward = 1
        elif player_final == dealer_final:
            result = 'DRAW'
            reward = 0
        else:
            result = 'LOSS'
            reward = -1

        return {
            'result': result,
            'reward': reward,
            'player_final': player_final,
            'dealer_final': dealer_final,
            'history': game_history
        }

    @staticmethod
    def get_basic_strategy_action(player_value, dealer_card, has_usable_ace):
        """
        –ë–∞–∑–æ–≤–∞—è –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–ø—Ç–∏–º–∞–ª—å–Ω–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è Blackjack.
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∫–∞–∫ –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è.
        """
        # –ë–∞–∑–æ–≤–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è (—É–ø—Ä–æ—â—ë–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)
        if has_usable_ace:
            # –ú—è–≥–∫–∞—è —Ä—É–∫–∞
            if player_value < 15:
                return 1  # HIT
            elif player_value == 15:
                return 1 if dealer_card >= 4 else 0
            elif player_value == 16:
                return 1 if dealer_card >= 4 else 0
            elif player_value == 17:
                return 1 if dealer_card in [2, 3, 4, 5, 6] else 0
            elif player_value == 18:
                return 0  # STAND
            else:
                return 0  # STAND
        else:
            # –ñ—ë—Å—Ç–∫–∞—è —Ä—É–∫–∞
            if player_value <= 11:
                return 1  # HIT
            elif player_value == 12:
                return 1 if dealer_card in [2, 3, 7, 8, 9, 10, 11] else 0
            elif player_value in [13, 14, 15, 16]:
                return 1 if dealer_card in [7, 8, 9, 10, 11] else 0
            else:
                return 0  # STAND

# ============================================================================
# –ö–õ–ê–°–°: Q-Learning Agent —Å Deep Neural Network
# ============================================================================

class DeepQAgent:
    """
    –ê–≥–µ–Ω—Ç Q-Learning —Å –≥–ª—É–±–æ–∫–æ–π –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç—å—é.
    –ò–∑—É—á–∞–µ—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é Blackjack –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–ø—ã—Ç–∞ –∏–≥—Ä—ã.
    """

    def __init__(self,
                 input_size=3,
                 hidden1=128,
                 hidden2=64,
                 output_size=2,
                 activation='relu',
                 dropout_rate=0.2,
                 learning_rate=0.001,
                 optimizer='adam',
                 loss_function='mse'):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–≥–µ–Ω—Ç–∞.

        Args:
            input_size: –†–∞–∑–º–µ—Ä –≤—Ö–æ–¥–Ω–æ–≥–æ —Å–ª–æ—è (3 –ø–∞—Ä–∞–º–µ—Ç—Ä–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è)
            hidden1: –†–∞–∑–º–µ—Ä –ø–µ—Ä–≤–æ–≥–æ —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–ª–æ—è
            hidden2: –†–∞–∑–º–µ—Ä –≤—Ç–æ—Ä–æ–≥–æ —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–ª–æ—è
            output_size: –†–∞–∑–º–µ—Ä –≤—ã—Ö–æ–¥–Ω–æ–≥–æ —Å–ª–æ—è (2 –¥–µ–π—Å—Ç–≤–∏—è)
            activation: –§—É–Ω–∫—Ü–∏—è –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ —Å–∫—Ä—ã—Ç—ã—Ö —Å–ª–æ—ë–≤
            dropout_rate: –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç dropout –¥–ª—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏
            learning_rate: –°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è
            optimizer: –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä (adam, rmsprop, sgd)
            loss_function: –§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å (mse, mae)
        """
        self.input_size = input_size
        self.output_size = output_size
        self.learning_rate = learning_rate
        self.epsilon = 1.0  # –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è
        self.epsilon_decay = 0.995
        self.epsilon_min = 0.01
        self.gamma = 0.95  # –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –¥–∏—Å–∫–æ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        self.memory = deque(maxlen=2000)  # –ü–∞–º—è—Ç—å –¥–ª—è Replay Buffer
        self.q_values_history = defaultdict(lambda: [0, 0])  # –¢–∞–±–ª–∏—á–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ Q-–∑–Ω–∞—á–µ–Ω–∏–π

        # –°–æ–∑–¥–∞–Ω–∏–µ –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏
        self.model = self._build_model(
            input_size, hidden1, hidden2, output_size,
            activation, dropout_rate, learning_rate, optimizer, loss_function
        )

        print(f"‚úì Deep Q-Network —Å–æ–∑–¥–∞–Ω–∞:")
        print(f"  - –í—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π: {input_size} –Ω–µ–π—Ä–æ–Ω–æ–≤")
        print(f"  - –°–∫—Ä—ã—Ç—ã–π —Å–ª–æ–π 1: {hidden1} –Ω–µ–π—Ä–æ–Ω–æ–≤ ({activation})")
        print(f"  - –°–∫—Ä—ã—Ç—ã–π —Å–ª–æ–π 2: {hidden2} –Ω–µ–π—Ä–æ–Ω–æ–≤ ({activation})")
        print(f"  - –í—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π: {output_size} –Ω–µ–π—Ä–æ–Ω–æ–≤ (Q-values)")
        print(f"  - Optimizer: {optimizer}, Learning Rate: {learning_rate}")
        print(f"  - Dropout Rate: {dropout_rate}")
        print()

    def _build_model(self, input_size, hidden1, hidden2, output_size,
                     activation, dropout_rate, lr, optimizer, loss_fn):
        """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏."""
        model = keras.Sequential([
            layers.Input(shape=(input_size,)),

            # –ü–µ—Ä–≤—ã–π —Å–ª–æ–π
            layers.Dense(hidden1, activation=activation,
                        kernel_regularizer=keras.regularizers.l2(0.001),
                        name='dense_1'),
            layers.Dropout(dropout_rate),
            layers.BatchNormalization(),

            # –í—Ç–æ—Ä–æ–π —Å–ª–æ–π
            layers.Dense(hidden2, activation=activation,
                        kernel_regularizer=keras.regularizers.l2(0.001),
                        name='dense_2'),
            layers.Dropout(dropout_rate),
            layers.BatchNormalization(),

            # –í—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π
            layers.Dense(output_size, activation='linear', name='q_values')
        ])

        # –ö–æ–º–ø–∏–ª—è—Ü–∏—è –º–æ–¥–µ–ª–∏
        if optimizer.lower() == 'adam':
            opt = keras.optimizers.Adam(learning_rate=lr)
        elif optimizer.lower() == 'rmsprop':
            opt = keras.optimizers.RMSprop(learning_rate=lr)
        else:
            opt = keras.optimizers.SGD(learning_rate=lr)

        if loss_fn.lower() == 'mse':
            loss = 'mse'
        else:
            loss = 'mae'

        model.compile(optimizer=opt, loss=loss, metrics=['mae'])

        return model

    def encode_state(self, state):
        """–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è –≤ –º–∞—Å—Å–∏–≤ –¥–ª—è –Ω–µ–π—Ä–æ—Å–µ—Ç–∏."""
        player_value, dealer_card, has_usable_ace = state
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∑–Ω–∞—á–µ–Ω–∏–π
        return np.array([
            (player_value - 12) / 9,  # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è 12-21 –≤ 0-1
            (dealer_card - 1) / 10,   # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è 1-10 –≤ 0-1
            float(has_usable_ace)     # 0 –∏–ª–∏ 1
        ])

    def select_action(self, state, training=True):
        """
        –í—ã–±–æ—Ä –¥–µ–π—Å—Ç–≤–∏—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º epsilon-greedy —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏.

        Args:
            state: –¢–µ–∫—É—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –∏–≥—Ä—ã
            training: –†–µ–∂–∏–º –æ–±—É—á–µ–Ω–∏—è (True) –∏–ª–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è (False)

        Returns:
            int: –ù–æ–º–µ—Ä –¥–µ–π—Å—Ç–≤–∏—è (0=Stand, 1=Hit)
        """
        # –í —Ä–µ–∂–∏–º–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –≤—Å–µ–≥–¥–∞ –≤—ã–±–∏—Ä–∞–µ–º –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ
        if not training:
            return self.get_best_action(state)

        # Epsilon-greedy –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ
        if random.random() < self.epsilon:
            return random.choice([0, 1])
        else:
            return self.get_best_action(state)

    def get_best_action(self, state):
        """–í—ã–±–æ—Ä –ª—É—á—à–µ–≥–æ –¥–µ–π—Å—Ç–≤–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ Q-–∑–Ω–∞—á–µ–Ω–∏–π."""
        encoded = self.encode_state(state)
        encoded = np.reshape(encoded, [1, self.input_size])
        q_values = self.model.predict(encoded, verbose=0)
        return np.argmax(q_values[0])

    def remember(self, state, action, reward, next_state, done):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–ø—ã—Ç–∞ –≤ –ø–∞–º—è—Ç—å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è."""
        self.memory.append((state, action, reward, next_state, done))

    def replay(self, batch_size):
        """
        –û–±—É—á–µ–Ω–∏–µ –Ω–∞ –±–∞—Ç—á–µ –∏–∑ –ø–∞–º—è—Ç–∏ (Experience Replay).

        Args:
            batch_size: –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è

        Returns:
            float: –°—Ä–µ–¥–Ω—è—è –ø–æ—Ç–µ—Ä—è –Ω–∞ –±–∞—Ç—á–µ
        """
        if len(self.memory) < batch_size:
            batch_size = len(self.memory)

        batch = random.sample(self.memory, batch_size)

        states = np.array([self.encode_state(x[0]) for x in batch])
        actions = np.array([x[1] for x in batch])
        rewards = np.array([x[2] for x in batch])
        next_states = np.array([self.encode_state(x[3]) for x in batch])
        dones = np.array([x[4] for x in batch])

        # –†–∞—Å—á—ë—Ç target Q-values
        target_q_values = self.model.predict(states, verbose=0)
        next_q_values = self.model.predict(next_states, verbose=0)

        for i in range(batch_size):
            if dones[i]:
                target_q_values[i][actions[i]] = rewards[i]
            else:
                target_q_values[i][actions[i]] = rewards[i] + self.gamma * np.max(next_q_values[i])

        # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∏ –ø–æ–ª—É—á–µ–Ω–∏–µ loss
        history = self.model.train_on_batch(states, target_q_values)

        # history —ç—Ç–æ —Å–ø–∏—Å–æ–∫ [loss, metrics], –±–µ—Ä—ë–º –ø–µ—Ä–≤—ã–π —ç–ª–µ–º–µ–Ω—Ç
        return history if isinstance(history, (int, float)) else history[0]

    def decay_epsilon(self):
        """–°–Ω–∏–∂–µ–Ω–∏–µ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–∞ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è."""
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay

# ============================================================================
# –ö–õ–ê–°–°: –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–∏–µ–º –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π
# ============================================================================

class BlackjackTrainer:
    """–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–æ—Ü–µ—Å—Å–æ–º –æ–±—É—á–µ–Ω–∏—è –∏ —Å–±–æ—Ä —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏."""

    def __init__(self, agent, num_epochs=100, num_games=2000, batch_size=32):
        """
        Args:
            agent: –≠–∫–∑–µ–º–ø–ª—è—Ä DeepQAgent
            num_epochs: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö –æ–±—É—á–µ–Ω–∏—è
            num_games: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏–≥—Ä –Ω–∞ —ç–ø–æ—Ö—É
            batch_size: –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        """
        self.agent = agent
        self.num_epochs = num_epochs
        self.num_games = num_games
        self.batch_size = batch_size

        self.history = {
            'epoch': [],
            'loss': [],
            'epsilon': [],
            'win_rate': [],
            'avg_reward': []
        }

    def train(self):
        """–û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è."""
        game = BlackjackGame(num_decks=1)

        print("üéÆ –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏...")
        print(f"   –≠–ø–æ—Ö: {self.num_epochs} | –ò–≥—Ä –Ω–∞ —ç–ø–æ—Ö—É: {self.num_games}")
        print(f"   Batch size: {self.batch_size}\n")

        for epoch in range(self.num_epochs):
            epoch_loss = 0.0
            loss_count = 0
            wins = 0
            total_reward = 0

            # –¢—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞ –Ω–∞ —Å–µ—Ä–∏–∏ –∏–≥—Ä
            for game_num in range(self.num_games):
                # –°—Ç—Ä–∞—Ç–µ–≥–∏—è –∞–≥–µ–Ω—Ç–∞
                def strategy(state):
                    return self.agent.select_action(state, training=True)

                # –ò–≥—Ä–∞–µ–º –∏–≥—Ä—É
                result = game.play_game(agent_strategy=strategy)
                reward = result['reward']

                # –£—á–∏–º –º–æ–¥–µ–ª—å –Ω–∞ –∫–∞–∂–¥–æ–º —à–∞–≥–µ
                for i, (state, action) in enumerate(result['history']):
                    # –°–ª–µ–¥—É—é—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –∏–ª–∏ —Ç–µ—Ä–º–∏–Ω–∞–ª—å–Ω–æ–µ
                    if i < len(result['history']) - 1:
                        next_state = result['history'][i + 1][0]
                        done = False
                    else:
                        next_state = state  # –§–∏–Ω–∞–ª—å–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
                        done = True

                    self.agent.remember(state, action, reward, next_state, done)

                # –û–±—É—á–µ–Ω–∏–µ –Ω–∞ –±–∞—Ç—á–µ
                if len(self.agent.memory) > self.batch_size:
                    loss = self.agent.replay(self.batch_size)
                    epoch_loss += loss
                    loss_count += 1

                # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
                if result['reward'] == 1:
                    wins += 1
                total_reward += result['reward']

            # –°–Ω–∏–∂–µ–Ω–∏–µ epsilon
            self.agent.decay_epsilon()

            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏
            win_rate = (wins / self.num_games) * 100
            avg_reward = total_reward / self.num_games
            avg_loss = epoch_loss / loss_count if loss_count > 0 else 0

            self.history['epoch'].append(epoch + 1)
            self.history['loss'].append(avg_loss)
            self.history['epsilon'].append(self.agent.epsilon)
            self.history['win_rate'].append(win_rate)
            self.history['avg_reward'].append(avg_reward)

            # –í—ã–≤–æ–¥ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
            if (epoch + 1) % max(1, self.num_epochs // 10) == 0 or epoch == 0:
                print(f"–≠–ø–æ—Ö–∞ {epoch + 1}/{self.num_epochs} | "
                      f"Loss: {avg_loss:.4f} | "
                      f"Win Rate: {win_rate:.2f}% | "
                      f"Epsilon: {self.agent.epsilon:.4f}")

        print("\n‚úì –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!\n")
        return self.history

    def evaluate(self, num_games=1000):
        """–û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –∏–≥—Ä–∞—Ö."""
        game = BlackjackGame(num_decks=1)

        results = {
            'wins': 0,
            'losses': 0,
            'draws': 0,
            'total_reward': 0
        }

        print(f"üß™ –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ –Ω–∞ {num_games} –∏–≥—Ä...")

        for _ in range(num_games):
            def strategy(state):
                return self.agent.select_action(state, training=False)

            result = game.play_game(agent_strategy=strategy)

            if result['result'] == 'WIN':
                results['wins'] += 1
            elif result['result'] == 'LOSS':
                results['losses'] += 1
            else:
                results['draws'] += 1

            results['total_reward'] += result['reward']

        # –†–∞—Å—á—ë—Ç –º–µ—Ç—Ä–∏–∫
        win_rate = (results['wins'] / num_games) * 100
        loss_rate = (results['losses'] / num_games) * 100
        draw_rate = (results['draws'] / num_games) * 100
        avg_reward = results['total_reward'] / num_games

        print(f"   –ü–æ–±–µ–¥—ã: {results['wins']} ({win_rate:.2f}%)")
        print(f"   –ü–æ—Ä–∞–∂–µ–Ω–∏—è: {results['losses']} ({loss_rate:.2f}%)")
        print(f"   –ù–∏—á—å–∏: {results['draws']} ({draw_rate:.2f}%)")
        print(f"   –°—Ä–µ–¥–Ω–∏–π –≤—ã–∏–≥—Ä—ã—à: {avg_reward:.4f}\n")

        return results

# ============================================================================
# –§–£–ù–ö–¶–ò–ò –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–ò
# ============================================================================

def plot_training_history(history):
    """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∏—Å—Ç–æ—Ä–∏–∏ –æ–±—É—á–µ–Ω–∏—è."""
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    fig.suptitle('–ò—Å—Ç–æ—Ä–∏—è –æ–±—É—á–µ–Ω–∏—è Deep Q-Network Blackjack', fontsize=16, fontweight='bold')

    # Loss
    axes[0, 0].plot(history['epoch'], history['loss'], 'b-', linewidth=2)
    axes[0, 0].set_title('Loss –ø–æ —ç–ø–æ—Ö–∞–º', fontsize=12, fontweight='bold')
    axes[0, 0].set_xlabel('–≠–ø–æ—Ö–∞')
    axes[0, 0].set_ylabel('Loss')
    axes[0, 0].grid(True, alpha=0.3)

    # Win Rate
    axes[0, 1].plot(history['epoch'], history['win_rate'], 'g-', linewidth=2)
    axes[0, 1].set_title('–ü—Ä–æ—Ü–µ–Ω—Ç –ø–æ–±–µ–¥ –ø–æ —ç–ø–æ—Ö–∞–º', fontsize=12, fontweight='bold')
    axes[0, 1].set_xlabel('–≠–ø–æ—Ö–∞')
    axes[0, 1].set_ylabel('Win Rate (%)')
    axes[0, 1].grid(True, alpha=0.3)
    axes[0, 1].axhline(y=48.5, color='r', linestyle='--', label='–ë–∞–∑–æ–≤–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è')
    axes[0, 1].legend()

    # Epsilon Decay
    axes[1, 0].plot(history['epoch'], history['epsilon'], 'r-', linewidth=2)
    axes[1, 0].set_title('Decay –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–∞ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è', fontsize=12, fontweight='bold')
    axes[1, 0].set_xlabel('–≠–ø–æ—Ö–∞')
    axes[1, 0].set_ylabel('Epsilon')
    axes[1, 0].grid(True, alpha=0.3)

    # Average Reward
    axes[1, 1].plot(history['epoch'], history['avg_reward'], 'm-', linewidth=2)
    axes[1, 1].set_title('–°—Ä–µ–¥–Ω–∏–π –≤—ã–∏–≥—Ä—ã—à –ø–æ —ç–ø–æ—Ö–∞–º', fontsize=12, fontweight='bold')
    axes[1, 1].set_xlabel('–≠–ø–æ—Ö–∞')
    axes[1, 1].set_ylabel('Average Reward')
    axes[1, 1].grid(True, alpha=0.3)
    axes[1, 1].axhline(y=0, color='k', linestyle='-', alpha=0.3)

    plt.tight_layout()
    plt.show()

def create_strategy_table(agent):
    """–°–æ–∑–¥–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏."""
    dealer_cards = list(range(1, 11))
    player_sums = list(range(5, 22))

    # –¢–∞–±–ª–∏—Ü–∞ –±–µ–∑ –º—è–≥–∫–æ–≥–æ —Ç—É–∑–∞ (Hard hands)
    strategy_hard = np.zeros((len(player_sums), len(dealer_cards)), dtype=int)

    for i, player_sum in enumerate(player_sums):
        for j, dealer_card in enumerate(dealer_cards):
            state = (player_sum, dealer_card, False)
            action = agent.get_best_action(state)
            strategy_hard[i, j] = action

    # –¢–∞–±–ª–∏—Ü–∞ —Å –º—è–≥–∫–∏–º —Ç—É–∑–æ–º (Soft hands)
    strategy_soft = np.zeros((8, len(dealer_cards)), dtype=int)

    for i, player_sum in enumerate(range(13, 21)):
        for j, dealer_card in enumerate(dealer_cards):
            state = (player_sum, dealer_card, True)
            action = agent.get_best_action(state)
            strategy_soft[i, j] = action

    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
    fig, axes = plt.subplots(1, 2, figsize=(16, 6))
    fig.suptitle('–û–ø—Ç–∏–º–∞–ª—å–Ω–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è Blackjack', fontsize=16, fontweight='bold')

    # Hard hands
    sns.heatmap(strategy_hard, annot=True, fmt='d', cmap='RdYlGn',
                xticklabels=['A', '2', '3', '4', '5', '6', '7', '8', '9', '10'],
                yticklabels=[f'{s}' for s in player_sums],
                cbar_kws={'label': '0=Stand, 1=Hit'}, ax=axes[0])
    axes[0].set_title('–ñ—ë—Å—Ç–∫–∏–µ —Ä—É–∫–∏ (Hard Hands)', fontsize=12, fontweight='bold')
    axes[0].set_xlabel('–û—Ç–∫—Ä—ã—Ç–∞—è –∫–∞—Ä—Ç–∞ –¥–∏–ª–µ—Ä–∞')
    axes[0].set_ylabel('–°—É–º–º–∞ –∫–∞—Ä—Ç –∏–≥—Ä–æ–∫–∞')

    # Soft hands
    sns.heatmap(strategy_soft, annot=True, fmt='d', cmap='RdYlGn',
                xticklabels=['A', '2', '3', '4', '5', '6', '7', '8', '9', '10'],
                yticklabels=[f'–ú—è–≥–∫–∞—è {s}' for s in range(13, 21)],
                cbar_kws={'label': '0=Stand, 1=Hit'}, ax=axes[1])
    axes[1].set_title('–ú—è–≥–∫–∏–µ —Ä—É–∫–∏ (Soft Hands)', fontsize=12, fontweight='bold')
    axes[1].set_xlabel('–û—Ç–∫—Ä—ã—Ç–∞—è –∫–∞—Ä—Ç–∞ –¥–∏–ª–µ—Ä–∞')
    axes[1].set_ylabel('–°—É–º–º–∞ –∫–∞—Ä—Ç –∏–≥—Ä–æ–∫–∞')

    plt.tight_layout()
    plt.show()

    return strategy_hard, strategy_soft

def compare_strategies(agent, num_games=1000):
    """–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –∞–≥–µ–Ω—Ç–∞ —Å –±–∞–∑–æ–≤–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–µ–π."""
    game = BlackjackGame(num_decks=1)

    # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–≥–µ–Ω—Ç–∞
    agent_wins = 0
    agent_losses = 0
    agent_draws = 0

    # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –±–∞–∑–æ–≤–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏
    basic_wins = 0
    basic_losses = 0
    basic_draws = 0

    print(f"üîÑ –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–π –Ω–∞ {num_games} –∏–≥—Ä–∞—Ö...")

    for _ in range(num_games):
        # –ò–≥—Ä–∞ —Å –∞–≥–µ–Ω—Ç–æ–º
        def agent_strategy(state):
            return agent.select_action(state, training=False)

        agent_result = game.play_game(agent_strategy=agent_strategy)

        # –ò–≥—Ä–∞ —Å –±–∞–∑–æ–≤–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–µ–π
        basic_result = game.play_game(use_basic_strategy=True)

        # –ü–æ–¥—Å—á—ë—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        if agent_result['result'] == 'WIN':
            agent_wins += 1
        elif agent_result['result'] == 'LOSS':
            agent_losses += 1
        else:
            agent_draws += 1

        if basic_result['result'] == 'WIN':
            basic_wins += 1
        elif basic_result['result'] == 'LOSS':
            basic_losses += 1
        else:
            basic_draws += 1

    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
    fig, axes = plt.subplots(1, 2, figsize=(12, 5))
    fig.suptitle('–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–π', fontsize=14, fontweight='bold')

    # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–≥–µ–Ω—Ç–∞
    agent_results = [agent_wins, agent_losses, agent_draws]
    axes[0].bar(['–ü–æ–±–µ–¥—ã', '–ü–æ—Ä–∞–∂–µ–Ω–∏—è', '–ù–∏—á—å–∏'], agent_results, color=['green', 'red', 'gray'])
    axes[0].set_title(f'Deep Q-Network Agent\nWin Rate: {agent_wins/num_games*100:.2f}%', fontweight='bold')
    axes[0].set_ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏–≥—Ä')

    # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –±–∞–∑–æ–≤–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏
    basic_results = [basic_wins, basic_losses, basic_draws]
    axes[1].bar(['–ü–æ–±–µ–¥—ã', '–ü–æ—Ä–∞–∂–µ–Ω–∏—è', '–ù–∏—á—å–∏'], basic_results, color=['green', 'red', 'gray'])
    axes[1].set_title(f'Basic Strategy\nWin Rate: {basic_wins/num_games*100:.2f}%', fontweight='bold')
    axes[1].set_ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏–≥—Ä')

    plt.tight_layout()
    plt.show()

# ============================================================================
# –û–°–ù–û–í–ù–û–ô –ö–û–î –í–´–ü–û–õ–ù–ï–ù–ò–Ø
# ============================================================================

def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø—Ä–æ–≥—Ä–∞–º–º—ã."""

    print("\n" + "=" * 80)
    print(" –ü–ê–†–ê–ú–ï–¢–†–´ –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–ò –°–ò–°–¢–ï–ú–´")
    print("=" * 80 + "\n")

    # ========== –ü–ê–†–ê–ú–ï–¢–†–´ –ú–û–î–ï–õ–ò ==========
    print("üß† –ü–ê–†–ê–ú–ï–¢–†–´ –ù–ï–ô–†–û–ù–ù–û–ô –°–ï–¢–ò:")
    INPUT_SIZE = 3           # –í—Ö–æ–¥–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —Å–æ—Å—Ç–æ—è–Ω–∏—è
    HIDDEN_LAYER_1 = 128     # –ü–µ—Ä–≤—ã–π —Å–∫—Ä—ã—Ç—ã–π —Å–ª–æ–π
    HIDDEN_LAYER_2 = 64      # –í—Ç–æ—Ä–æ–π —Å–∫—Ä—ã—Ç—ã–π —Å–ª–æ–π
    OUTPUT_SIZE = 2          # –í—ã—Ö–æ–¥–Ω—ã–µ –¥–µ–π—Å—Ç–≤–∏—è (Stand/Hit)
    ACTIVATION = 'relu'      # –§—É–Ω–∫—Ü–∏—è –∞–∫—Ç–∏–≤–∞—Ü–∏–∏
    DROPOUT_RATE = 0.2       # –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç dropout

    print(f"  Input Size: {INPUT_SIZE}")
    print(f"  Hidden Layer 1: {HIDDEN_LAYER_1}")
    print(f"  Hidden Layer 2: {HIDDEN_LAYER_2}")
    print(f"  Output Size: {OUTPUT_SIZE}")
    print(f"  Activation: {ACTIVATION}")
    print(f"  Dropout: {DROPOUT_RATE}\n")

    # ========== –ü–ê–†–ê–ú–ï–¢–†–´ –û–ë–£–ß–ï–ù–ò–Ø ==========
    print("üìö –ü–ê–†–ê–ú–ï–¢–†–´ –û–ë–£–ß–ï–ù–ò–Ø:")
    LEARNING_RATE = 0.001    # –°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è
    BATCH_SIZE = 32          # –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞
    NUM_EPOCHS = 100          # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö
    GAMES_PER_EPOCH = 100    # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏–≥—Ä –Ω–∞ —ç–ø–æ—Ö—É
    DISCOUNT_FACTOR = 0.95   # –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –¥–∏—Å–∫–æ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
    OPTIMIZER = 'adam'       # –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä
    LOSS_FUNCTION = 'mse'    # –§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å

    print(f"  Learning Rate: {LEARNING_RATE}")
    print(f"  Batch Size: {BATCH_SIZE}")
    print(f"  Epochs: {NUM_EPOCHS}")
    print(f"  Games per Epoch: {GAMES_PER_EPOCH}")
    print(f"  Discount Factor: {DISCOUNT_FACTOR}")
    print(f"  Optimizer: {OPTIMIZER}")
    print(f"  Loss Function: {LOSS_FUNCTION}\n")

    # ========== –ü–ê–†–ê–ú–ï–¢–†–´ –ò–ì–†–´ ==========
    print("üé∞ –ü–ê–†–ê–ú–ï–¢–†–´ –ò–ì–†–´:")
    DECK_SIZE = 1            # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–æ–ª–æ–¥
    TRAIN_GAMES = 2500       # –ò–≥—Ä—ã –¥–ª—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
    TEST_GAMES = 2500   # –¢–µ—Å—Ç–æ–≤—ã–µ –∏–≥—Ä—ã

    print(f"  Deck Size: {DECK_SIZE}")
    print(f"  Train Games: {TRAIN_GAMES}")
    print(f"  Test Games: {TEST_GAMES}\n")

    print("=" * 80 + "\n")

    # –°–æ–∑–¥–∞–Ω–∏–µ –∞–≥–µ–Ω—Ç–∞
    agent = DeepQAgent(
        input_size=INPUT_SIZE,
        hidden1=HIDDEN_LAYER_1,
        hidden2=HIDDEN_LAYER_2,
        output_size=OUTPUT_SIZE,
        activation=ACTIVATION,
        dropout_rate=DROPOUT_RATE,
        learning_rate=LEARNING_RATE,
        optimizer=OPTIMIZER,
        loss_function=LOSS_FUNCTION
    )

    # –°–æ–∑–¥–∞–Ω–∏–µ —Ç—Ä–µ–Ω–µ—Ä–∞
    trainer = BlackjackTrainer(
        agent=agent,
        num_epochs=NUM_EPOCHS,
        num_games=GAMES_PER_EPOCH,
        batch_size=BATCH_SIZE
    )

    # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    history = trainer.train()

    # –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏
    print("=" * 80)
    print(" –†–ï–ó–£–õ–¨–¢–ê–¢–´ –û–¶–ï–ù–ö–ò")
    print("=" * 80 + "\n")
    results = trainer.evaluate(num_games=TEST_GAMES)

    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    print("\nüìä –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤...")
    plot_training_history(history)
    create_strategy_table(agent)
    compare_strategies(agent, num_games=TRAIN_GAMES)

    # –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    print("\n" + "=" * 80)
    print(" –ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê")
    print("=" * 80)
    print(f"‚úì –°–∏—Å—Ç–µ–º–∞ —É—Å–ø–µ—à–Ω–æ –æ–±—É—á–µ–Ω–∞!")
    print(f"  –≠–ø–æ—Ö: {NUM_EPOCHS}")
    print(f"  –í—Å–µ–≥–æ –∏–≥—Ä: {NUM_EPOCHS * GAMES_PER_EPOCH}")
    print(f"  Win Rate: {results['wins']/TEST_GAMES*100:.2f}%")
    print(f"  Loss Rate: {results['losses']/TEST_GAMES*100:.2f}%")
    print(f"  Draw Rate: {results['draws']/TEST_GAMES*100:.2f}%")
    print("=" * 80 + "\n")

    return agent, trainer, history

# ============================================================================
# –ó–ê–ü–£–°–ö –ü–†–û–ì–†–ê–ú–ú–´
# ============================================================================

if __name__ == "__main__":
    # –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è
    agent, trainer, history = main()

    print("\n‚úì –ü—Ä–æ–≥—Ä–∞–º–º–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")
    print("  –ú–æ–¥–µ–ª—å –≥–æ—Ç–æ–≤–∞ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é.")
    print("  –í—ã –º–æ–∂–µ—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å 'agent.select_action(state, training=False)'")
    print("  –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º–æ–≥–æ –¥–µ–π—Å—Ç–≤–∏—è –≤ –ª—é–±–æ–º —Å–æ—Å—Ç–æ—è–Ω–∏–∏ –∏–≥—Ä—ã.")
